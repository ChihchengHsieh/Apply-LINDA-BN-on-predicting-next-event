{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd00fab9f4e2480a02528c8c5e2c5eed9f6f178de25ad0614451a9194f538e2ccc5",
   "display_name": "Python 3.9.4 64-bit ('dl')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/jrhs/.pyenv/versions/dl/lib/python3.9/site-packages/pandas/compat/__init__.py:97: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from Controller.TrainingParameters import TrainingParameters\n",
    "from Utils.Constants import Constants\n",
    "from enum import Enum\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from Data.BPI2012Dataset import BPI2012Dataset\n",
    "from Models.BaselineLSMTModel import BaselineLSTMModel\n",
    "from Utils.Constants import Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotSupportedError(Exception):\n",
    "    pass\n",
    "\n",
    "class SelectableDatasets(Enum):\n",
    "    BPI2012 = 1\n",
    "    Helpdesk = 2\n",
    "\n",
    "class SelectableLoss(Enum):\n",
    "    CrossEntropy = 1\n",
    "\n",
    "class SelectableModels(Enum):\n",
    "    BaseLineLSTMModel = 1\n",
    "\n",
    "class SelectableOptimizer(Enum):\n",
    "    Adam = 1\n",
    "\n",
    "class TrainingController:\n",
    "    def __init__(self, dataset: SelectableDatasets, model: SelectableModels, optimizer: SelectableOptimizer, loss: SelectableLoss):\n",
    "    \n",
    "        ## Load dataset\n",
    "        if (dataset == SelectableDatasets.BPI2012):\n",
    "            self.dataset = BPI2012Dataset(TrainingParameters.bpi_2012_path)\n",
    "        else:\n",
    "            raise NotSupportedError(\"Dataset you selected is not supported\")\n",
    "\n",
    "        self.data_loader = DataLoader(self.dataset, batch_size=32, shuffle=True, collate_fn= self.dataset.collate_fn)\n",
    "\n",
    "        # Setting up model \n",
    "        if (model == SelectableModels.BaseLineLSTMModel):\n",
    "            self.model = BaselineLSTMModel(\n",
    "                vocab_size= self.dataset.vocab_size(),\n",
    "                embedding_dim= TrainingParameters.BaselineLSTMModelParameters.embedding_dim,\n",
    "                lstm_hidden= TrainingParameters.BaselineLSTMModelParameters.lstm_hidden,\n",
    "                dropout= TrainingParameters.BaselineLSTMModelParameters.dropout,\n",
    "                num_lstm_layers= TrainingParameters.BaselineLSTMModelParameters.num_lstm_layers,\n",
    "                paddingValue= self.dataset.vocab_to_index(Constants.PAD_VOCAB),\n",
    "            )\n",
    "        else:\n",
    "            raise NotSupportedError(\"Model you selected is not supported\")\n",
    "\n",
    "        # Setting up optimizer\n",
    "        if optimizer == SelectableOptimizer.Adam:\n",
    "            self.opt = optim.Adam(\n",
    "                self.model.parameters(),\n",
    "                lr =TrainingParameters.OptimizerParameters.learning_rate,\n",
    "                                weight_decay= TrainingParameters.OptimizerParameters.l2\n",
    "                                )\n",
    "        else: \n",
    "             raise NotSupportedError(\"Optimizer you selected is not supported\")\n",
    "\n",
    "        # Setting up loss\n",
    "\n",
    "        if (loss == SelectableLoss.CrossEntropy):\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            raise NotSupportedError(\"Loss function you selected is not supported\")\n",
    "        self.epoch = 0\n",
    "        self.steps = 0\n",
    "\n",
    "    def train(self, stop_epoch: int):\n",
    "        while self.epoch < stop_epoch: \n",
    "            self.model.train()\n",
    "            for i, (_, train, target, lengths) in enumerate(self.data_loader):\n",
    "                train, target, lengths = train.to(device), target.to(device), lengths.to(device)\n",
    "                loss, accuracy = self.train_step(train, target, lengths)\n",
    "                self.steps += 1\n",
    "\n",
    "                if self.steps % 5 == 0:\n",
    "                    print('| Epoch [%d] | Step [%d] | lr [%.6f] | Loss: [%.4f] | Acc: [%.4f]|' % (self.epoch, self.steps, self.opt.param_groups[0]['lr'], loss, accuracy))\n",
    "\n",
    "            self.epoch += 1\n",
    "\n",
    "    def train_step(self, train, target, lengths):\n",
    "        self.opt.zero_grad()\n",
    "        loss, accuracy = self.model_step(train, target, lengths)\n",
    "        loss.backward()\n",
    "        self.opt.step()\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "\n",
    "    def model_step(self, train, target, lengths):\n",
    "\n",
    "        out = self.model(train, lengths)\n",
    "\n",
    "        loss = self.loss(out.transpose(2,1), target)\n",
    "\n",
    "        accuracy = torch.mean((self.model.get_predicted_seq_from_output(out) == target).float())\n",
    "\n",
    "        return loss, accuracy\n",
    "\n",
    "    def reset_epoch(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def reset_steps(self):\n",
    "        self.steps = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 13087/13087 [00:16<00:00, 778.60it/s] \n",
      "/Users/jrhs/.pyenv/versions/dl/lib/python3.9/site-packages/pandas/core/frame.py:1549: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = TrainingController(\n",
    "    dataset= SelectableDatasets.BPI2012, model = SelectableModels.BaseLineLSTMModel, optimizer = SelectableOptimizer.Adam, loss = SelectableLoss.CrossEntropy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "| Epoch [0] | Step [5] | lr [0.005000] | Loss: [3.6699] | Acc: [0.0228]|\n",
      "| Epoch [0] | Step [10] | lr [0.005000] | Loss: [3.6398] | Acc: [0.0206]|\n",
      "| Epoch [0] | Step [15] | lr [0.005000] | Loss: [3.5920] | Acc: [0.0312]|\n",
      "| Epoch [0] | Step [20] | lr [0.005000] | Loss: [3.5496] | Acc: [0.0365]|\n",
      "| Epoch [0] | Step [25] | lr [0.005000] | Loss: [3.5323] | Acc: [0.0244]|\n",
      "| Epoch [0] | Step [30] | lr [0.005000] | Loss: [3.5156] | Acc: [0.7324]|\n",
      "| Epoch [0] | Step [35] | lr [0.005000] | Loss: [3.4747] | Acc: [0.8257]|\n",
      "| Epoch [0] | Step [40] | lr [0.005000] | Loss: [3.4488] | Acc: [0.7645]|\n",
      "| Epoch [0] | Step [45] | lr [0.005000] | Loss: [3.4237] | Acc: [0.7504]|\n",
      "| Epoch [0] | Step [50] | lr [0.005000] | Loss: [3.3967] | Acc: [0.7299]|\n",
      "| Epoch [0] | Step [55] | lr [0.005000] | Loss: [3.3884] | Acc: [0.7426]|\n",
      "| Epoch [0] | Step [60] | lr [0.005000] | Loss: [3.3465] | Acc: [0.7915]|\n",
      "| Epoch [0] | Step [65] | lr [0.005000] | Loss: [3.3384] | Acc: [0.7353]|\n",
      "| Epoch [0] | Step [70] | lr [0.005000] | Loss: [3.3190] | Acc: [0.7387]|\n",
      "| Epoch [0] | Step [75] | lr [0.005000] | Loss: [3.2762] | Acc: [0.7607]|\n",
      "| Epoch [0] | Step [80] | lr [0.005000] | Loss: [3.2969] | Acc: [0.6932]|\n",
      "| Epoch [0] | Step [85] | lr [0.005000] | Loss: [3.2446] | Acc: [0.7880]|\n",
      "| Epoch [0] | Step [90] | lr [0.005000] | Loss: [3.2356] | Acc: [0.7873]|\n",
      "| Epoch [0] | Step [95] | lr [0.005000] | Loss: [3.2253] | Acc: [0.7582]|\n",
      "| Epoch [0] | Step [100] | lr [0.005000] | Loss: [3.2309] | Acc: [0.6979]|\n",
      "| Epoch [0] | Step [105] | lr [0.005000] | Loss: [3.1317] | Acc: [0.8730]|\n",
      "| Epoch [0] | Step [110] | lr [0.005000] | Loss: [3.1578] | Acc: [0.7899]|\n",
      "| Epoch [0] | Step [115] | lr [0.005000] | Loss: [3.2026] | Acc: [0.6843]|\n",
      "| Epoch [0] | Step [120] | lr [0.005000] | Loss: [3.1305] | Acc: [0.7889]|\n",
      "| Epoch [0] | Step [125] | lr [0.005000] | Loss: [3.1734] | Acc: [0.6888]|\n",
      "| Epoch [0] | Step [130] | lr [0.005000] | Loss: [3.1190] | Acc: [0.7302]|\n",
      "| Epoch [0] | Step [135] | lr [0.005000] | Loss: [3.0959] | Acc: [0.7392]|\n",
      "| Epoch [0] | Step [140] | lr [0.005000] | Loss: [3.0955] | Acc: [0.7056]|\n",
      "| Epoch [0] | Step [145] | lr [0.005000] | Loss: [3.0579] | Acc: [0.7596]|\n",
      "| Epoch [0] | Step [150] | lr [0.005000] | Loss: [3.0808] | Acc: [0.6821]|\n",
      "| Epoch [0] | Step [155] | lr [0.005000] | Loss: [3.0938] | Acc: [0.6443]|\n",
      "| Epoch [0] | Step [160] | lr [0.005000] | Loss: [2.9736] | Acc: [0.8049]|\n",
      "| Epoch [0] | Step [165] | lr [0.005000] | Loss: [3.0132] | Acc: [0.7413]|\n",
      "| Epoch [0] | Step [170] | lr [0.005000] | Loss: [3.1176] | Acc: [0.5666]|\n",
      "| Epoch [0] | Step [175] | lr [0.005000] | Loss: [2.9071] | Acc: [0.8235]|\n",
      "| Epoch [0] | Step [180] | lr [0.005000] | Loss: [3.0127] | Acc: [0.6771]|\n",
      "| Epoch [0] | Step [185] | lr [0.005000] | Loss: [3.0172] | Acc: [0.6557]|\n",
      "| Epoch [0] | Step [190] | lr [0.005000] | Loss: [3.0073] | Acc: [0.6576]|\n",
      "| Epoch [0] | Step [195] | lr [0.005000] | Loss: [3.0127] | Acc: [0.6324]|\n",
      "| Epoch [0] | Step [200] | lr [0.005000] | Loss: [2.9495] | Acc: [0.7000]|\n",
      "| Epoch [0] | Step [205] | lr [0.005000] | Loss: [2.9755] | Acc: [0.6586]|\n",
      "| Epoch [0] | Step [210] | lr [0.005000] | Loss: [3.0000] | Acc: [0.6147]|\n",
      "| Epoch [0] | Step [215] | lr [0.005000] | Loss: [2.7668] | Acc: [0.8418]|\n",
      "| Epoch [0] | Step [220] | lr [0.005000] | Loss: [2.9528] | Acc: [0.6380]|\n",
      "| Epoch [0] | Step [225] | lr [0.005000] | Loss: [2.8103] | Acc: [0.7643]|\n",
      "| Epoch [0] | Step [230] | lr [0.005000] | Loss: [2.7184] | Acc: [0.8382]|\n",
      "| Epoch [0] | Step [235] | lr [0.005000] | Loss: [2.7727] | Acc: [0.7734]|\n",
      "| Epoch [0] | Step [240] | lr [0.005000] | Loss: [2.8608] | Acc: [0.6795]|\n",
      "| Epoch [0] | Step [245] | lr [0.005000] | Loss: [2.8744] | Acc: [0.6584]|\n",
      "| Epoch [0] | Step [250] | lr [0.005000] | Loss: [2.9048] | Acc: [0.6230]|\n",
      "| Epoch [0] | Step [255] | lr [0.005000] | Loss: [2.8852] | Acc: [0.6284]|\n",
      "| Epoch [0] | Step [260] | lr [0.005000] | Loss: [2.7346] | Acc: [0.7534]|\n",
      "| Epoch [0] | Step [265] | lr [0.005000] | Loss: [2.6910] | Acc: [0.7754]|\n",
      "| Epoch [0] | Step [270] | lr [0.005000] | Loss: [2.8045] | Acc: [0.6686]|\n",
      "| Epoch [0] | Step [275] | lr [0.005000] | Loss: [2.6858] | Acc: [0.7555]|\n",
      "| Epoch [0] | Step [280] | lr [0.005000] | Loss: [2.5299] | Acc: [0.8668]|\n",
      "| Epoch [0] | Step [285] | lr [0.005000] | Loss: [2.7660] | Acc: [0.6646]|\n",
      "| Epoch [0] | Step [290] | lr [0.005000] | Loss: [2.7572] | Acc: [0.6672]|\n",
      "| Epoch [0] | Step [295] | lr [0.005000] | Loss: [2.6845] | Acc: [0.7152]|\n",
      "| Epoch [0] | Step [300] | lr [0.005000] | Loss: [2.6748] | Acc: [0.7135]|\n",
      "| Epoch [0] | Step [305] | lr [0.005000] | Loss: [2.8183] | Acc: [0.6003]|\n",
      "| Epoch [0] | Step [310] | lr [0.005000] | Loss: [2.6622] | Acc: [0.7054]|\n",
      "| Epoch [0] | Step [315] | lr [0.005000] | Loss: [2.7943] | Acc: [0.6071]|\n",
      "| Epoch [0] | Step [320] | lr [0.005000] | Loss: [2.5194] | Acc: [0.7869]|\n",
      "| Epoch [0] | Step [325] | lr [0.005000] | Loss: [2.7062] | Acc: [0.6509]|\n",
      "| Epoch [0] | Step [330] | lr [0.005000] | Loss: [2.7407] | Acc: [0.6234]|\n",
      "| Epoch [0] | Step [335] | lr [0.005000] | Loss: [2.4954] | Acc: [0.7774]|\n",
      "| Epoch [0] | Step [340] | lr [0.005000] | Loss: [2.5736] | Acc: [0.7174]|\n",
      "| Epoch [0] | Step [345] | lr [0.005000] | Loss: [2.4621] | Acc: [0.7820]|\n",
      "| Epoch [0] | Step [350] | lr [0.005000] | Loss: [2.4529] | Acc: [0.7775]|\n",
      "| Epoch [0] | Step [355] | lr [0.005000] | Loss: [2.6444] | Acc: [0.6534]|\n",
      "| Epoch [0] | Step [360] | lr [0.005000] | Loss: [2.7115] | Acc: [0.6026]|\n",
      "| Epoch [0] | Step [365] | lr [0.005000] | Loss: [2.5775] | Acc: [0.6824]|\n",
      "| Epoch [0] | Step [370] | lr [0.005000] | Loss: [2.7174] | Acc: [0.5918]|\n",
      "| Epoch [0] | Step [375] | lr [0.005000] | Loss: [2.5959] | Acc: [0.6602]|\n",
      "| Epoch [0] | Step [380] | lr [0.005000] | Loss: [2.5199] | Acc: [0.6999]|\n",
      "| Epoch [0] | Step [385] | lr [0.005000] | Loss: [2.4218] | Acc: [0.7520]|\n",
      "| Epoch [0] | Step [390] | lr [0.005000] | Loss: [2.5331] | Acc: [0.6825]|\n",
      "| Epoch [0] | Step [395] | lr [0.005000] | Loss: [2.4078] | Acc: [0.7443]|\n",
      "| Epoch [0] | Step [400] | lr [0.005000] | Loss: [2.4696] | Acc: [0.7024]|\n",
      "| Epoch [0] | Step [405] | lr [0.005000] | Loss: [2.5500] | Acc: [0.6549]|\n"
     ]
    }
   ],
   "source": [
    "trainer.train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.tensor([[1,1], [1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6ed379317d9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "torch.mean((a == b).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}